{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit Score v1",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangithajk/Hackathon/blob/master/Credit_Score_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D96BYS0z-YNe",
        "colab_type": "code",
        "outputId": "25d75cfa-29d2-42e5-c1aa-5e324c9c95f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJvmQMq3--Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvjpXH8A_I8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/CreditScore_train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/CreditScore_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6J-v3V2_XyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"source\"] = \"train\"\n",
        "test[\"source\"] = \"test\"\n",
        "df = pd.concat([train,test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvC9VIfN_xkp",
        "colab_type": "code",
        "outputId": "c1f8d62f-69de-4718-823a-e9fce11bc875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x001</th>\n",
              "      <th>x002</th>\n",
              "      <th>x003</th>\n",
              "      <th>x004</th>\n",
              "      <th>x005</th>\n",
              "      <th>x006</th>\n",
              "      <th>x007</th>\n",
              "      <th>x008</th>\n",
              "      <th>x009</th>\n",
              "      <th>x010</th>\n",
              "      <th>x011</th>\n",
              "      <th>x012</th>\n",
              "      <th>x013</th>\n",
              "      <th>x014</th>\n",
              "      <th>x015</th>\n",
              "      <th>x016</th>\n",
              "      <th>x017</th>\n",
              "      <th>x018</th>\n",
              "      <th>x019</th>\n",
              "      <th>x020</th>\n",
              "      <th>x021</th>\n",
              "      <th>x022</th>\n",
              "      <th>x023</th>\n",
              "      <th>x024</th>\n",
              "      <th>x025</th>\n",
              "      <th>x026</th>\n",
              "      <th>x027</th>\n",
              "      <th>x028</th>\n",
              "      <th>x029</th>\n",
              "      <th>x030</th>\n",
              "      <th>x031</th>\n",
              "      <th>x032</th>\n",
              "      <th>x033</th>\n",
              "      <th>x034</th>\n",
              "      <th>x035</th>\n",
              "      <th>x036</th>\n",
              "      <th>x037</th>\n",
              "      <th>x038</th>\n",
              "      <th>x039</th>\n",
              "      <th>x040</th>\n",
              "      <th>...</th>\n",
              "      <th>x267</th>\n",
              "      <th>x268</th>\n",
              "      <th>x269</th>\n",
              "      <th>x270</th>\n",
              "      <th>x271</th>\n",
              "      <th>x272</th>\n",
              "      <th>x273</th>\n",
              "      <th>x274</th>\n",
              "      <th>x275</th>\n",
              "      <th>x276</th>\n",
              "      <th>x277</th>\n",
              "      <th>x278</th>\n",
              "      <th>x279</th>\n",
              "      <th>x280</th>\n",
              "      <th>x281</th>\n",
              "      <th>x282</th>\n",
              "      <th>x283</th>\n",
              "      <th>x284</th>\n",
              "      <th>x285</th>\n",
              "      <th>x286</th>\n",
              "      <th>x287</th>\n",
              "      <th>x288</th>\n",
              "      <th>x289</th>\n",
              "      <th>x290</th>\n",
              "      <th>x291</th>\n",
              "      <th>x292</th>\n",
              "      <th>x293</th>\n",
              "      <th>x294</th>\n",
              "      <th>x295</th>\n",
              "      <th>x296</th>\n",
              "      <th>x297</th>\n",
              "      <th>x298</th>\n",
              "      <th>x299</th>\n",
              "      <th>x300</th>\n",
              "      <th>x301</th>\n",
              "      <th>x302</th>\n",
              "      <th>x303</th>\n",
              "      <th>x304</th>\n",
              "      <th>y</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1084094</td>\n",
              "      <td>426.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1943</td>\n",
              "      <td>484289</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>484289</td>\n",
              "      <td>0</td>\n",
              "      <td>484289</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>346762</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>807</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1287777</td>\n",
              "      <td>160.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.8417</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2356</td>\n",
              "      <td>125307</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125307</td>\n",
              "      <td>0</td>\n",
              "      <td>125307</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>17318</td>\n",
              "      <td>124634</td>\n",
              "      <td>0.8417</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17318</td>\n",
              "      <td>0.8417</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>819</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1483016</td>\n",
              "      <td>163.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0308</td>\n",
              "      <td>706</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>706</td>\n",
              "      <td>0</td>\n",
              "      <td>706</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>803</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>959054</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>102.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.3025</td>\n",
              "      <td>619</td>\n",
              "      <td>619</td>\n",
              "      <td>0.8123</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>530</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1342113</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0180</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>38837</td>\n",
              "      <td>21424</td>\n",
              "      <td>0.7357</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17839</td>\n",
              "      <td>426</td>\n",
              "      <td>17413</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17413</td>\n",
              "      <td>17413</td>\n",
              "      <td>1.0180</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17413</td>\n",
              "      <td>1.0180</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>485</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 306 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      x001   x002  x003   x004   x005  ...  x302  x303  x304    y  source\n",
              "0  1084094  426.0  39.0  128.0  426.0  ...   NaN     0   NaN  807   train\n",
              "1  1287777  160.0   2.0   64.0  160.0  ...   NaN     0   NaN  819   train\n",
              "2  1483016  163.0  16.0  104.0  239.0  ...   NaN     0   NaN  803   train\n",
              "3   959054    NaN   NaN    NaN  102.0  ...   NaN     0   NaN  530   train\n",
              "4  1342113    3.0   2.0    2.0   62.0  ...   NaN     0   NaN  485   train\n",
              "\n",
              "[5 rows x 306 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHPMbkGZ_3AN",
        "colab_type": "code",
        "outputId": "32416fb6-e1e6-4392-9850-63e44013e99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 306)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfjGS-KM_6UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDKlL_FUMqRx",
        "colab_type": "code",
        "outputId": "087513af-8a07-480f-8772-c5791499e027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x001          0\n",
              "x002      21432\n",
              "x003      21432\n",
              "x004      21424\n",
              "x005       6110\n",
              "x006          0\n",
              "x007          0\n",
              "x008          0\n",
              "x009          0\n",
              "x010          0\n",
              "x011          0\n",
              "x012          0\n",
              "x013          0\n",
              "x014          0\n",
              "x015          0\n",
              "x016          0\n",
              "x017          0\n",
              "x018          0\n",
              "x019          0\n",
              "x020          0\n",
              "x021          0\n",
              "x022          0\n",
              "x023          0\n",
              "x024          0\n",
              "x025          0\n",
              "x026          0\n",
              "x027          0\n",
              "x028          0\n",
              "x029          0\n",
              "x030          0\n",
              "          ...  \n",
              "x277          0\n",
              "x278          0\n",
              "x279          0\n",
              "x280          0\n",
              "x281          0\n",
              "x282          0\n",
              "x283          0\n",
              "x284          0\n",
              "x285          0\n",
              "x286          0\n",
              "x287      24821\n",
              "x288      49756\n",
              "x289      49756\n",
              "x290      49756\n",
              "x291          0\n",
              "x292          0\n",
              "x293      51133\n",
              "x294          0\n",
              "x295      86533\n",
              "x296          0\n",
              "x297      58112\n",
              "x298          0\n",
              "x299          0\n",
              "x300          0\n",
              "x301          0\n",
              "x302      73069\n",
              "x303          0\n",
              "x304      81875\n",
              "y             0\n",
              "source        0\n",
              "Length: 306, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwSwSuv-ACW7",
        "colab_type": "code",
        "outputId": "fb456afe-3307-4191-f27e-f8d3750dea05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "for i in range(306) :\n",
        "  if df.iloc[:,i].isna().sum() > 50000:\n",
        "    print(i+1)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98\n",
            "155\n",
            "162\n",
            "242\n",
            "253\n",
            "255\n",
            "256\n",
            "257\n",
            "259\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "275\n",
            "293\n",
            "295\n",
            "297\n",
            "302\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll-9ymOJM5Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['x001','x098','x155','x162','x242','x253','x255','x256','x257','x259','x265','x266','x267','x268','x275','x293','x295','x297','x302','x304'],inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bPBHCw3OEUg",
        "colab_type": "code",
        "outputId": "d263d3aa-5d4f-4c2a-f52c-41b73443853e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#for i in range(286) :\n",
        "#  print( df.iloc[:,i].isna().sum() )\n",
        "for i in df.columns:\n",
        "  if df.loc[:,i].isna().sum() > 0:\n",
        "    print(i)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x002\n",
            "x003\n",
            "x004\n",
            "x005\n",
            "x041\n",
            "x044\n",
            "x045\n",
            "x057\n",
            "x058\n",
            "x148\n",
            "x222\n",
            "x223\n",
            "x234\n",
            "x235\n",
            "x237\n",
            "x238\n",
            "x239\n",
            "x272\n",
            "x287\n",
            "x288\n",
            "x289\n",
            "x290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfldfmtSO19o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in df.columns:\n",
        "  if i not in [ 'source'] :\n",
        "    df[i].fillna(df[i].median(),inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4cQehRTwd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split    \n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvceNhCUUco3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_final = df[df.source==\"train\"]\n",
        "test_final = df[df.source==\"test\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsfcsB2RVROj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = train_final.drop(['y','source'],axis=1)\n",
        "y_train = train_final['y']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhNms4JmV3Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = test_final.drop(['y','source'],axis=1)\n",
        "y_test = test_final['y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDuI5drmVoNj",
        "colab_type": "code",
        "outputId": "1611dbb7-78bd-4d82-f1f0-da1b0aca23b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80000, 284) (80000,) (20000, 284) (20000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GSN8ISCWHSm",
        "colab_type": "code",
        "outputId": "bee116fc-26c1-4d24-b021-a5679e2b8d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.linear_model import  LinearRegression\n",
        "linear = LinearRegression().fit(x_train,y_train)\n",
        "lr_train_predict = linear.predict(x_train)\n",
        "lr_test_predict = linear.predict(x_test)\n",
        "from sklearn import metrics\n",
        "print(\"MAE value of rf train:\",metrics.mean_absolute_error(y_train,lr_train_predict))\n",
        "print(\"MAE value of rf test:\",metrics.mean_absolute_error(y_test,lr_test_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE value of rf train: 35.64575905184995\n",
            "MAE value of rf test: 35.74629970859242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mvcu3UIWohk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mape_find(y_true,y_predict):\n",
        "    y_true, y_predict = np.array(y_true) , np.array(y_predict)\n",
        "    return np.mean(np.abs((y_true - y_predict)/y_true))*100\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_2caykWs2C",
        "colab_type": "code",
        "outputId": "00101679-4e49-4c37-cafe-2c86e6afb3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "err_df = pd.DataFrame(columns=['Algo','Test MAPE','Train MAPE'])\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,lr_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,lr_test_predict))\n",
        "\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,lr_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,lr_test_predict)))\n",
        "\n",
        "err_df.loc[0] = ['Linear', np.sqrt(metrics.mean_squared_error(y_test,lr_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,lr_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE value of rf train: 6.15414616712415\n",
            "MAPE value of rf test: 6.188509451063522\n",
            "RMSE value of rf train: 47.17941667688385\n",
            "RMSE value of rf test: 47.136840790316654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84z4PUMeXBA7",
        "colab_type": "code",
        "outputId": "5b5668ab-2523-498f-cf8d-99f8ea57339b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtree = DecisionTreeRegressor(max_depth=3).fit(x_train,y_train)\n",
        "dtree_train_predict = dtree.predict(x_train)\n",
        "dtree_test_predict = dtree.predict(x_test)\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,dtree_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,dtree_test_predict))\n",
        "\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,dtree_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,dtree_test_predict)))\n",
        "\n",
        "err_df.loc[1] = ['Dtree', np.sqrt(metrics.mean_squared_error(y_test,dtree_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,dtree_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE value of rf train: 8.049060796510009\n",
            "MAPE value of rf test: 8.081355431962969\n",
            "RMSE value of rf train: 60.50427951031923\n",
            "RMSE value of rf test: 60.42786992357211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dx1558hXP9I",
        "colab_type": "code",
        "outputId": "2072ef33-6f73-4d2b-c835-5ba8b9bcec70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.ensemble import  RandomForestRegressor\n",
        "forest = RandomForestRegressor(n_jobs=-1,n_estimators=20, \n",
        "                                       min_samples_leaf=25, \n",
        "                                       max_features=0.3).fit(x_train, y_train)\n",
        "\n",
        "forest_train_predict = forest.predict(x_train)\n",
        "forest_test_predict = forest.predict(x_test)\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,forest_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,forest_test_predict))\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,forest_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,forest_test_predict)))\n",
        "\n",
        "err_df.loc[2] = ['RF', np.sqrt(metrics.mean_squared_error(y_test,forest_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,forest_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE value of rf train: 3.374434935796415\n",
            "MAPE value of rf test: 3.757086430409183\n",
            "RMSE value of rf train: 27.482052135997353\n",
            "RMSE value of rf test: 30.651829809771723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIjS4oCyXejf",
        "colab_type": "code",
        "outputId": "be85254a-cdc6-4649-c718-5c0ba3f58817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn import metrics\n",
        "\n",
        "xgb_fit = xgb.XGBRegressor(max_depth=2, n_estimators=100, learning_rate=0.1).fit(x_train, y_train)\n",
        "xgb_train_predict = xgb_fit.predict(x_train)\n",
        "xgb_test_predict = xgb_fit.predict(x_test)\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,xgb_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,xgb_test_predict))\n",
        "\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,xgb_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,xgb_test_predict)))\n",
        "\n",
        "err_df.loc[3] = ['XGB', np.sqrt(metrics.mean_squared_error(y_test,xgb_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,xgb_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  if getattr(data, 'base', None) is not None and \\\n",
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  data.base is not None and isinstance(data, np.ndarray) \\\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[06:07:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "MAPE value of rf train: 4.586118089414084\n",
            "MAPE value of rf test: 4.62887706906609\n",
            "RMSE value of rf train: 35.835422417066205\n",
            "RMSE value of rf test: 36.04916205739765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZp1jvItXnz4",
        "colab_type": "code",
        "outputId": "839caf8a-ddd4-4331-9651-1f0d25636a58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "for K in range(15):\n",
        "    K_value = K+1\n",
        "    neigh = KNeighborsRegressor(n_neighbors = K_value, weights='uniform', algorithm='auto')\n",
        "    neigh.fit(x_train, y_train) \n",
        "    y_pred = neigh.predict(x_test)\n",
        "    print (\"RMSE is \", np.sqrt(metrics.mean_squared_error(y_test,y_pred)),\"% for K-Value:\",K_value)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE is  65.23434294909393 % for K-Value: 1\n",
            "RMSE is  58.09081683020131 % for K-Value: 2\n",
            "RMSE is  55.47028133253978 % for K-Value: 3\n",
            "RMSE is  54.31722298221808 % for K-Value: 4\n",
            "RMSE is  53.67153646021325 % for K-Value: 5\n",
            "RMSE is  53.389777553593746 % for K-Value: 6\n",
            "RMSE is  53.14969318551412 % for K-Value: 7\n",
            "RMSE is  53.034758038066414 % for K-Value: 8\n",
            "RMSE is  52.91953851058076 % for K-Value: 9\n",
            "RMSE is  52.85063790059681 % for K-Value: 10\n",
            "RMSE is  52.82825441039659 % for K-Value: 11\n",
            "RMSE is  52.85237383308026 % for K-Value: 12\n",
            "RMSE is  52.89513811675448 % for K-Value: 13\n",
            "RMSE is  52.924815027199344 % for K-Value: 14\n",
            "RMSE is  52.960126363561905 % for K-Value: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5SZxvKfxwV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_df.loc[4] = ['KNN', 52.85, 52]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cb5J8FVX9SU",
        "colab_type": "code",
        "outputId": "9a9088a8-4b88-4aad-cbc5-c7d18750e268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lass = Lasso().fit(x_train, y_train)\n",
        "lass_train_predict = lass.predict(x_train)\n",
        "lass_test_predict = lass.predict(x_test)\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,lass_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,lass_test_predict))\n",
        "\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,lass_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,lass_test_predict)))\n",
        "\n",
        "err_df.loc[5] = ['Lasso', np.sqrt(metrics.mean_squared_error(y_test,lass_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,lass_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE value of rf train: 6.5703023942845595\n",
            "MAPE value of rf test: 6.558936588441863\n",
            "RMSE value of rf train: 49.79862320194668\n",
            "RMSE value of rf test: 49.784222674807474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117569600.60720114, tolerance: 112066.04657994871\n",
            "  positive)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucSN5ma3YH0I",
        "colab_type": "code",
        "outputId": "1ac7d20b-392c-490b-f847-402822c57a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "rid = Ridge().fit(x_train, y_train)\n",
        "rid_train_predict = rid.predict(x_train)\n",
        "rid_test_predict = rid.predict(x_test)\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,rid_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,rid_test_predict))\n",
        "\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,rid_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,rid_test_predict)))\n",
        "\n",
        "err_df.loc[6] = ['Ridge', np.sqrt(metrics.mean_squared_error(y_test,rid_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,rid_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE value of rf train: 6.154171631443435\n",
            "MAPE value of rf test: 6.188506154713017\n",
            "RMSE value of rf train: 47.179417834269174\n",
            "RMSE value of rf test: 47.13672938274046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.5498e-17): result may not be accurate.\n",
            "  overwrite_a=True).T\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMiGIdm3YNov",
        "colab_type": "code",
        "outputId": "e04b4dec-d86d-43b6-9d7f-569b90d4d79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "en = ElasticNet().fit(x_train, y_train)\n",
        "en_train_predict = en.predict(x_train)\n",
        "en_test_predict = en.predict(x_test)\n",
        "print(\"MAPE value of rf train:\",mape_find(y_train,en_train_predict))\n",
        "print(\"MAPE value of rf test:\",mape_find(y_test,en_test_predict))\n",
        "\n",
        "print(\"RMSE value of rf train:\",np.sqrt(metrics.mean_squared_error(y_train,en_train_predict)))\n",
        "print(\"RMSE value of rf test:\",np.sqrt(metrics.mean_squared_error(y_test,en_test_predict)))\n",
        "\n",
        "err_df.loc[7] = ['EN', np.sqrt(metrics.mean_squared_error(y_test,en_test_predict)) , np.sqrt(metrics.mean_squared_error(y_train,en_train_predict))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE value of rf train: 7.067173042826383\n",
            "MAPE value of rf test: 7.051872933404107\n",
            "RMSE value of rf train: 53.20190140666271\n",
            "RMSE value of rf test: 53.33090380117717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 131965059.52001265, tolerance: 112066.04657994871\n",
            "  positive)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qth7k8FTYY8j",
        "colab_type": "code",
        "outputId": "39aba21a-1204-42f2-9fe3-9e5cef199247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "import seaborn as sns\n",
        "print(err_df)\n",
        "sns.barplot(x=err_df.Algo,y=err_df['Test MAPE'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Algo  Test MAPE  Train MAPE\n",
            "0  Linear  47.136841   47.179417\n",
            "1   Dtree  60.427870   60.504280\n",
            "2      RF  30.651830   27.482052\n",
            "3     XGB  36.049162   35.835422\n",
            "4     KNN  52.850000   52.000000\n",
            "5   Lasso  49.784223   49.798623\n",
            "6   Ridge  47.136729   47.179418\n",
            "7      EN  53.330904   53.201901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd105031668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+/92uo5bimVJPwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQBiEp5YC4Z4",
        "colab_type": "code",
        "outputId": "e17104bc-fae2-4f77-96db-cd5fddaa6cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 284)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnPF4foPY5Nu",
        "colab_type": "code",
        "outputId": "665d2188-9ed3-46ef-a317-2b1bc7daebf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(20, input_dim=284))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dense(50))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dense(30))\n",
        "model.add(Activation('elu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mse'])\n",
        "\n",
        "history = model.fit(x_train, y_train, validation_split = 0.3, epochs = 150, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 56000 samples, validate on 24000 samples\n",
            "Epoch 1/150\n",
            "56000/56000 [==============================] - 4s 72us/step - loss: 566092.8940 - mean_squared_error: 566092.8940 - val_loss: 2281303.5164 - val_mean_squared_error: 2281303.5164\n",
            "Epoch 2/150\n",
            "56000/56000 [==============================] - 4s 66us/step - loss: 168973.5255 - mean_squared_error: 168973.5255 - val_loss: 87225.8562 - val_mean_squared_error: 87225.8562\n",
            "Epoch 3/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 107618.6562 - mean_squared_error: 107618.6562 - val_loss: 80795.3949 - val_mean_squared_error: 80795.3949\n",
            "Epoch 4/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 77536.4990 - mean_squared_error: 77536.4990 - val_loss: 49101.5241 - val_mean_squared_error: 49101.5241\n",
            "Epoch 5/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 54427.9612 - mean_squared_error: 54427.9612 - val_loss: 34961.1671 - val_mean_squared_error: 34961.1671\n",
            "Epoch 6/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 53682.1125 - mean_squared_error: 53682.1125 - val_loss: 46730.1647 - val_mean_squared_error: 46730.1647\n",
            "Epoch 7/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 41687.3782 - mean_squared_error: 41687.3782 - val_loss: 29112.8950 - val_mean_squared_error: 29112.8950\n",
            "Epoch 8/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 22499.0250 - mean_squared_error: 22499.0250 - val_loss: 9380.1211 - val_mean_squared_error: 9380.1211\n",
            "Epoch 9/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 7823.7551 - mean_squared_error: 7823.7551 - val_loss: 6415.9000 - val_mean_squared_error: 6415.9000\n",
            "Epoch 10/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 5428.2044 - mean_squared_error: 5428.2044 - val_loss: 5324.8473 - val_mean_squared_error: 5324.8473\n",
            "Epoch 11/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 9720.8479 - mean_squared_error: 9720.8479 - val_loss: 4531.2117 - val_mean_squared_error: 4531.2117\n",
            "Epoch 12/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 4363.5912 - mean_squared_error: 4363.5912 - val_loss: 4000.0483 - val_mean_squared_error: 4000.0483\n",
            "Epoch 13/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 4016.1555 - mean_squared_error: 4016.1555 - val_loss: 3482.2102 - val_mean_squared_error: 3482.2102\n",
            "Epoch 14/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 4265.5278 - mean_squared_error: 4265.5278 - val_loss: 3853.8931 - val_mean_squared_error: 3853.8931\n",
            "Epoch 15/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 3479.5158 - mean_squared_error: 3479.5158 - val_loss: 2950.5060 - val_mean_squared_error: 2950.5060\n",
            "Epoch 16/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2831.4453 - mean_squared_error: 2831.4453 - val_loss: 2452.1440 - val_mean_squared_error: 2452.1440\n",
            "Epoch 17/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 3375.9605 - mean_squared_error: 3375.9605 - val_loss: 2757.1092 - val_mean_squared_error: 2757.1092\n",
            "Epoch 18/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 4024.8237 - mean_squared_error: 4024.8237 - val_loss: 5767.0729 - val_mean_squared_error: 5767.0729\n",
            "Epoch 19/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 13005.4817 - mean_squared_error: 13005.4817 - val_loss: 3681.1834 - val_mean_squared_error: 3681.1834\n",
            "Epoch 20/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2691.1299 - mean_squared_error: 2691.1299 - val_loss: 2899.1214 - val_mean_squared_error: 2899.1214\n",
            "Epoch 21/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2359.8692 - mean_squared_error: 2359.8692 - val_loss: 2106.6756 - val_mean_squared_error: 2106.6756\n",
            "Epoch 22/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 2143.3943 - mean_squared_error: 2143.3943 - val_loss: 2126.1754 - val_mean_squared_error: 2126.1754\n",
            "Epoch 23/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2126.2894 - mean_squared_error: 2126.2894 - val_loss: 2034.2410 - val_mean_squared_error: 2034.2410\n",
            "Epoch 24/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1966.8710 - mean_squared_error: 1966.8710 - val_loss: 1843.8675 - val_mean_squared_error: 1843.8675\n",
            "Epoch 25/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1930.1943 - mean_squared_error: 1930.1943 - val_loss: 1856.5677 - val_mean_squared_error: 1856.5677\n",
            "Epoch 26/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1858.7941 - mean_squared_error: 1858.7941 - val_loss: 2049.1878 - val_mean_squared_error: 2049.1878\n",
            "Epoch 27/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1827.0028 - mean_squared_error: 1827.0028 - val_loss: 1812.8849 - val_mean_squared_error: 1812.8849\n",
            "Epoch 28/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1766.6160 - mean_squared_error: 1766.6160 - val_loss: 1699.8118 - val_mean_squared_error: 1699.8118\n",
            "Epoch 29/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2364.5503 - mean_squared_error: 2364.5503 - val_loss: 2070.0871 - val_mean_squared_error: 2070.0871\n",
            "Epoch 30/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1923.5950 - mean_squared_error: 1923.5950 - val_loss: 1852.0007 - val_mean_squared_error: 1852.0007\n",
            "Epoch 31/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1837.8286 - mean_squared_error: 1837.8286 - val_loss: 1884.7342 - val_mean_squared_error: 1884.7342\n",
            "Epoch 32/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1776.2154 - mean_squared_error: 1776.2154 - val_loss: 1698.9563 - val_mean_squared_error: 1698.9563\n",
            "Epoch 33/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 10725.2793 - mean_squared_error: 10725.2793 - val_loss: 2543.8026 - val_mean_squared_error: 2543.8026\n",
            "Epoch 34/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 2192.7611 - mean_squared_error: 2192.7611 - val_loss: 2071.6627 - val_mean_squared_error: 2071.6627\n",
            "Epoch 35/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2034.2446 - mean_squared_error: 2034.2446 - val_loss: 1850.4382 - val_mean_squared_error: 1850.4382\n",
            "Epoch 36/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1949.7764 - mean_squared_error: 1949.7764 - val_loss: 1983.9187 - val_mean_squared_error: 1983.9187\n",
            "Epoch 37/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1975.6118 - mean_squared_error: 1975.6118 - val_loss: 1791.2164 - val_mean_squared_error: 1791.2164\n",
            "Epoch 38/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1752.6734 - mean_squared_error: 1752.6734 - val_loss: 1682.1566 - val_mean_squared_error: 1682.1566\n",
            "Epoch 39/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 2028.7249 - mean_squared_error: 2028.7249 - val_loss: 1773.7796 - val_mean_squared_error: 1773.7796\n",
            "Epoch 40/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1847.4274 - mean_squared_error: 1847.4274 - val_loss: 2134.2098 - val_mean_squared_error: 2134.2098\n",
            "Epoch 41/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1734.8891 - mean_squared_error: 1734.8891 - val_loss: 1680.3632 - val_mean_squared_error: 1680.3632\n",
            "Epoch 42/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1643.2589 - mean_squared_error: 1643.2589 - val_loss: 1620.5255 - val_mean_squared_error: 1620.5255\n",
            "Epoch 43/150\n",
            "56000/56000 [==============================] - 4s 71us/step - loss: 6717.6873 - mean_squared_error: 6717.6873 - val_loss: 1815.5762 - val_mean_squared_error: 1815.5762\n",
            "Epoch 44/150\n",
            "56000/56000 [==============================] - 4s 66us/step - loss: 2039.2784 - mean_squared_error: 2039.2784 - val_loss: 1896.1029 - val_mean_squared_error: 1896.1029\n",
            "Epoch 45/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1740.5822 - mean_squared_error: 1740.5822 - val_loss: 1734.1791 - val_mean_squared_error: 1734.1791\n",
            "Epoch 46/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1682.0981 - mean_squared_error: 1682.0981 - val_loss: 1677.1530 - val_mean_squared_error: 1677.1530\n",
            "Epoch 47/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1627.6300 - mean_squared_error: 1627.6300 - val_loss: 80708302.6536 - val_mean_squared_error: 80708302.6536\n",
            "Epoch 48/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1598.1284 - mean_squared_error: 1598.1284 - val_loss: 1549.9959 - val_mean_squared_error: 1549.9959\n",
            "Epoch 49/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1553.2521 - mean_squared_error: 1553.2521 - val_loss: 1562.5952 - val_mean_squared_error: 1562.5952\n",
            "Epoch 50/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 2687.5233 - mean_squared_error: 2687.5233 - val_loss: 1618.8825 - val_mean_squared_error: 1618.8825\n",
            "Epoch 51/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1558.9938 - mean_squared_error: 1558.9938 - val_loss: 1794.4629 - val_mean_squared_error: 1794.4629\n",
            "Epoch 52/150\n",
            "56000/56000 [==============================] - 4s 66us/step - loss: 1655.1362 - mean_squared_error: 1655.1362 - val_loss: 1671.4048 - val_mean_squared_error: 1671.4048\n",
            "Epoch 53/150\n",
            "56000/56000 [==============================] - 4s 69us/step - loss: 1617.0010 - mean_squared_error: 1617.0010 - val_loss: 1533.5969 - val_mean_squared_error: 1533.5969\n",
            "Epoch 54/150\n",
            "56000/56000 [==============================] - 4s 69us/step - loss: 1530.5243 - mean_squared_error: 1530.5243 - val_loss: 1786.8501 - val_mean_squared_error: 1786.8501\n",
            "Epoch 55/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1517.6259 - mean_squared_error: 1517.6259 - val_loss: 1587.3950 - val_mean_squared_error: 1587.3950\n",
            "Epoch 56/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1497.3002 - mean_squared_error: 1497.3002 - val_loss: 1623.7032 - val_mean_squared_error: 1623.7032\n",
            "Epoch 57/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1577.8556 - mean_squared_error: 1577.8556 - val_loss: 1563.5363 - val_mean_squared_error: 1563.5363\n",
            "Epoch 58/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1477.3601 - mean_squared_error: 1477.3601 - val_loss: 1474.5228 - val_mean_squared_error: 1474.5228\n",
            "Epoch 59/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 2464.5431 - mean_squared_error: 2464.5431 - val_loss: 2346.4989 - val_mean_squared_error: 2346.4989\n",
            "Epoch 60/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1820.8425 - mean_squared_error: 1820.8425 - val_loss: 1639.8666 - val_mean_squared_error: 1639.8666\n",
            "Epoch 61/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1492.7498 - mean_squared_error: 1492.7498 - val_loss: 1573.5466 - val_mean_squared_error: 1573.5466\n",
            "Epoch 62/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1477.1619 - mean_squared_error: 1477.1619 - val_loss: 1680.9590 - val_mean_squared_error: 1680.9590\n",
            "Epoch 63/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1496.4545 - mean_squared_error: 1496.4545 - val_loss: 1479.2837 - val_mean_squared_error: 1479.2837\n",
            "Epoch 64/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1503.7551 - mean_squared_error: 1503.7551 - val_loss: 1448.4269 - val_mean_squared_error: 1448.4269\n",
            "Epoch 65/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1468.7385 - mean_squared_error: 1468.7385 - val_loss: 1370.0502 - val_mean_squared_error: 1370.0502\n",
            "Epoch 66/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1414.7073 - mean_squared_error: 1414.7073 - val_loss: 1404.7312 - val_mean_squared_error: 1404.7312\n",
            "Epoch 67/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 6002.3801 - mean_squared_error: 6002.3801 - val_loss: 1498.0804 - val_mean_squared_error: 1498.0804\n",
            "Epoch 68/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1449.7831 - mean_squared_error: 1449.7831 - val_loss: 1435.7897 - val_mean_squared_error: 1435.7897\n",
            "Epoch 69/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1403.2994 - mean_squared_error: 1403.2994 - val_loss: 1393.5144 - val_mean_squared_error: 1393.5144\n",
            "Epoch 70/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1375.3274 - mean_squared_error: 1375.3274 - val_loss: 1422.6164 - val_mean_squared_error: 1422.6164\n",
            "Epoch 71/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1358.1782 - mean_squared_error: 1358.1782 - val_loss: 1346.9525 - val_mean_squared_error: 1346.9525\n",
            "Epoch 72/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1355.1973 - mean_squared_error: 1355.1973 - val_loss: 1504.8990 - val_mean_squared_error: 1504.8990\n",
            "Epoch 73/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1351.1119 - mean_squared_error: 1351.1119 - val_loss: 1580.3213 - val_mean_squared_error: 1580.3213\n",
            "Epoch 74/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1351.1000 - mean_squared_error: 1351.1000 - val_loss: 1408.9579 - val_mean_squared_error: 1408.9579\n",
            "Epoch 75/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1385.3177 - mean_squared_error: 1385.3177 - val_loss: 3820.5846 - val_mean_squared_error: 3820.5846\n",
            "Epoch 76/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1343.1913 - mean_squared_error: 1343.1913 - val_loss: 1415.2758 - val_mean_squared_error: 1415.2758\n",
            "Epoch 77/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1346.6232 - mean_squared_error: 1346.6232 - val_loss: 1401.9219 - val_mean_squared_error: 1401.9219\n",
            "Epoch 78/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1458.7087 - mean_squared_error: 1458.7087 - val_loss: 1429.3798 - val_mean_squared_error: 1429.3798\n",
            "Epoch 79/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1440.1350 - mean_squared_error: 1440.1350 - val_loss: 1723.7239 - val_mean_squared_error: 1723.7239\n",
            "Epoch 80/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 11941.1134 - mean_squared_error: 11941.1134 - val_loss: 2008.5166 - val_mean_squared_error: 2008.5166\n",
            "Epoch 81/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1751.4622 - mean_squared_error: 1751.4622 - val_loss: 1610.5948 - val_mean_squared_error: 1610.5948\n",
            "Epoch 82/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1578.7898 - mean_squared_error: 1578.7898 - val_loss: 1590.4227 - val_mean_squared_error: 1590.4227\n",
            "Epoch 83/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1791.3737 - mean_squared_error: 1791.3737 - val_loss: 1622.4639 - val_mean_squared_error: 1622.4639\n",
            "Epoch 84/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1512.4919 - mean_squared_error: 1512.4919 - val_loss: 1455.8057 - val_mean_squared_error: 1455.8057\n",
            "Epoch 85/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1447.2923 - mean_squared_error: 1447.2923 - val_loss: 1443.6729 - val_mean_squared_error: 1443.6729\n",
            "Epoch 86/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1660.1284 - mean_squared_error: 1660.1284 - val_loss: 1693.3134 - val_mean_squared_error: 1693.3134\n",
            "Epoch 87/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1515.9725 - mean_squared_error: 1515.9725 - val_loss: 1419.0777 - val_mean_squared_error: 1419.0777\n",
            "Epoch 88/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1407.0642 - mean_squared_error: 1407.0642 - val_loss: 1396.4505 - val_mean_squared_error: 1396.4505\n",
            "Epoch 89/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1863.4109 - mean_squared_error: 1863.4109 - val_loss: 1451.3455 - val_mean_squared_error: 1451.3455\n",
            "Epoch 90/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1500164481.8214 - mean_squared_error: 1500164481.8214 - val_loss: 1931.4775 - val_mean_squared_error: 1931.4775\n",
            "Epoch 91/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1687.7018 - mean_squared_error: 1687.7018 - val_loss: 1662.9297 - val_mean_squared_error: 1662.9297\n",
            "Epoch 92/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1623.8686 - mean_squared_error: 1623.8686 - val_loss: 1635.3836 - val_mean_squared_error: 1635.3836\n",
            "Epoch 93/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1522.6454 - mean_squared_error: 1522.6454 - val_loss: 1513.5102 - val_mean_squared_error: 1513.5102\n",
            "Epoch 94/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1519.8239 - mean_squared_error: 1519.8239 - val_loss: 1643.1803 - val_mean_squared_error: 1643.1803\n",
            "Epoch 95/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1597.9830 - mean_squared_error: 1597.9830 - val_loss: 1532.8791 - val_mean_squared_error: 1532.8791\n",
            "Epoch 96/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1501.5091 - mean_squared_error: 1501.5091 - val_loss: 1508.4512 - val_mean_squared_error: 1508.4512\n",
            "Epoch 97/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1697.9229 - mean_squared_error: 1697.9229 - val_loss: 1534.2232 - val_mean_squared_error: 1534.2232\n",
            "Epoch 98/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1549.0862 - mean_squared_error: 1549.0862 - val_loss: 1562.2787 - val_mean_squared_error: 1562.2787\n",
            "Epoch 99/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1511.2887 - mean_squared_error: 1511.2887 - val_loss: 1547.5280 - val_mean_squared_error: 1547.5280\n",
            "Epoch 100/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1501.8409 - mean_squared_error: 1501.8409 - val_loss: 1676.5852 - val_mean_squared_error: 1676.5852\n",
            "Epoch 101/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1495.1710 - mean_squared_error: 1495.1710 - val_loss: 1495.0102 - val_mean_squared_error: 1495.0102\n",
            "Epoch 102/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1420.3602 - mean_squared_error: 1420.3602 - val_loss: 1413.4454 - val_mean_squared_error: 1413.4454\n",
            "Epoch 103/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1426.8353 - mean_squared_error: 1426.8353 - val_loss: 1412.1907 - val_mean_squared_error: 1412.1907\n",
            "Epoch 104/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1399.2737 - mean_squared_error: 1399.2737 - val_loss: 1535.0865 - val_mean_squared_error: 1535.0865\n",
            "Epoch 105/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1416.7682 - mean_squared_error: 1416.7682 - val_loss: 1414.9110 - val_mean_squared_error: 1414.9110\n",
            "Epoch 106/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1424.8478 - mean_squared_error: 1424.8478 - val_loss: 1761.4245 - val_mean_squared_error: 1761.4245\n",
            "Epoch 107/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1429.7455 - mean_squared_error: 1429.7455 - val_loss: 1361.0802 - val_mean_squared_error: 1361.0802\n",
            "Epoch 108/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1372.4375 - mean_squared_error: 1372.4375 - val_loss: 1396.1887 - val_mean_squared_error: 1396.1887\n",
            "Epoch 109/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1350.6900 - mean_squared_error: 1350.6900 - val_loss: 1422.9240 - val_mean_squared_error: 1422.9240\n",
            "Epoch 110/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1329.0348 - mean_squared_error: 1329.0348 - val_loss: 1362.8419 - val_mean_squared_error: 1362.8419\n",
            "Epoch 111/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1314.6131 - mean_squared_error: 1314.6131 - val_loss: 1443.7185 - val_mean_squared_error: 1443.7185\n",
            "Epoch 112/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1330.3793 - mean_squared_error: 1330.3793 - val_loss: 1422.3059 - val_mean_squared_error: 1422.3059\n",
            "Epoch 113/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 805507.7726 - mean_squared_error: 805507.7726 - val_loss: 1421.1618 - val_mean_squared_error: 1421.1618\n",
            "Epoch 114/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1406.5323 - mean_squared_error: 1406.5323 - val_loss: 1448.4185 - val_mean_squared_error: 1448.4185\n",
            "Epoch 115/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1359.1397 - mean_squared_error: 1359.1397 - val_loss: 1435.5188 - val_mean_squared_error: 1435.5188\n",
            "Epoch 116/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1286.8415 - mean_squared_error: 1286.8415 - val_loss: 1291.7150 - val_mean_squared_error: 1291.7150\n",
            "Epoch 117/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1309.3033 - mean_squared_error: 1309.3033 - val_loss: 1414.2950 - val_mean_squared_error: 1414.2950\n",
            "Epoch 118/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1296.5443 - mean_squared_error: 1296.5443 - val_loss: 1271.3497 - val_mean_squared_error: 1271.3497\n",
            "Epoch 119/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1279.3886 - mean_squared_error: 1279.3886 - val_loss: 1354.9869 - val_mean_squared_error: 1354.9869\n",
            "Epoch 120/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1247.7461 - mean_squared_error: 1247.7461 - val_loss: 1267.5784 - val_mean_squared_error: 1267.5784\n",
            "Epoch 121/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1255.2573 - mean_squared_error: 1255.2573 - val_loss: 1362.1793 - val_mean_squared_error: 1362.1793\n",
            "Epoch 122/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1252.7954 - mean_squared_error: 1252.7954 - val_loss: 1443.1721 - val_mean_squared_error: 1443.1721\n",
            "Epoch 123/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1250.7448 - mean_squared_error: 1250.7448 - val_loss: 1422.6927 - val_mean_squared_error: 1422.6927\n",
            "Epoch 124/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1257.9143 - mean_squared_error: 1257.9143 - val_loss: 1305.6134 - val_mean_squared_error: 1305.6134\n",
            "Epoch 125/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1245.6001 - mean_squared_error: 1245.6001 - val_loss: 1242.8877 - val_mean_squared_error: 1242.8877\n",
            "Epoch 126/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1343.8875 - mean_squared_error: 1343.8875 - val_loss: 1244.3150 - val_mean_squared_error: 1244.3150\n",
            "Epoch 127/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1291.2807 - mean_squared_error: 1291.2807 - val_loss: 1391.3861 - val_mean_squared_error: 1391.3861\n",
            "Epoch 128/150\n",
            "56000/56000 [==============================] - 4s 69us/step - loss: 1297.6035 - mean_squared_error: 1297.6035 - val_loss: 1320.8670 - val_mean_squared_error: 1320.8670\n",
            "Epoch 129/150\n",
            "56000/56000 [==============================] - 4s 68us/step - loss: 1282.6822 - mean_squared_error: 1282.6822 - val_loss: 1223.0689 - val_mean_squared_error: 1223.0689\n",
            "Epoch 130/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1274.4866 - mean_squared_error: 1274.4866 - val_loss: 1271.1108 - val_mean_squared_error: 1271.1108\n",
            "Epoch 131/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1313.1654 - mean_squared_error: 1313.1654 - val_loss: 21170.2017 - val_mean_squared_error: 21170.2017\n",
            "Epoch 132/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1339.7325 - mean_squared_error: 1339.7325 - val_loss: 7035.2570 - val_mean_squared_error: 7035.2570\n",
            "Epoch 133/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 89432.7393 - mean_squared_error: 89432.7393 - val_loss: 1805.6958 - val_mean_squared_error: 1805.6958\n",
            "Epoch 134/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1684.3646 - mean_squared_error: 1684.3646 - val_loss: 1662.2911 - val_mean_squared_error: 1662.2911\n",
            "Epoch 135/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1602.1216 - mean_squared_error: 1602.1216 - val_loss: 1610.7480 - val_mean_squared_error: 1610.7480\n",
            "Epoch 136/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1462.5083 - mean_squared_error: 1462.5083 - val_loss: 1503.3978 - val_mean_squared_error: 1503.3978\n",
            "Epoch 137/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1454.0317 - mean_squared_error: 1454.0317 - val_loss: 1463.8408 - val_mean_squared_error: 1463.8408\n",
            "Epoch 138/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1441.7651 - mean_squared_error: 1441.7651 - val_loss: 1534.1100 - val_mean_squared_error: 1534.1100\n",
            "Epoch 139/150\n",
            "56000/56000 [==============================] - 4s 67us/step - loss: 1388.1045 - mean_squared_error: 1388.1045 - val_loss: 1466.3707 - val_mean_squared_error: 1466.3707\n",
            "Epoch 140/150\n",
            "56000/56000 [==============================] - 4s 70us/step - loss: 1593.3058 - mean_squared_error: 1593.3058 - val_loss: 1486.0056 - val_mean_squared_error: 1486.0056\n",
            "Epoch 141/150\n",
            "56000/56000 [==============================] - 4s 69us/step - loss: 1703.8124 - mean_squared_error: 1703.8124 - val_loss: 1811.4375 - val_mean_squared_error: 1811.4375\n",
            "Epoch 142/150\n",
            "56000/56000 [==============================] - 4s 65us/step - loss: 1529.3650 - mean_squared_error: 1529.3650 - val_loss: 1498.2046 - val_mean_squared_error: 1498.2046\n",
            "Epoch 143/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1439.4080 - mean_squared_error: 1439.4080 - val_loss: 1348.1364 - val_mean_squared_error: 1348.1364\n",
            "Epoch 144/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1471.3994 - mean_squared_error: 1471.3994 - val_loss: 1483.3650 - val_mean_squared_error: 1483.3650\n",
            "Epoch 145/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1729.9669 - mean_squared_error: 1729.9669 - val_loss: 1354.0195 - val_mean_squared_error: 1354.0195\n",
            "Epoch 146/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1351.7405 - mean_squared_error: 1351.7405 - val_loss: 1375.2754 - val_mean_squared_error: 1375.2754\n",
            "Epoch 147/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1299.7864 - mean_squared_error: 1299.7864 - val_loss: 1358.9390 - val_mean_squared_error: 1358.9390\n",
            "Epoch 148/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1268.3207 - mean_squared_error: 1268.3207 - val_loss: 1271.2245 - val_mean_squared_error: 1271.2245\n",
            "Epoch 149/150\n",
            "56000/56000 [==============================] - 4s 64us/step - loss: 1252.9157 - mean_squared_error: 1252.9157 - val_loss: 1272.8306 - val_mean_squared_error: 1272.8306\n",
            "Epoch 150/150\n",
            "56000/56000 [==============================] - 4s 63us/step - loss: 1262.1194 - mean_squared_error: 1262.1194 - val_loss: 1248.1906 - val_mean_squared_error: 1248.1906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PRLucbmewqg",
        "colab_type": "code",
        "outputId": "44644a7c-6bce-44a7-c4a7-09cfafea0cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['training', 'validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wshRpZw6Csro",
        "colab_type": "code",
        "outputId": "df6ddb0e-c9ad-452a-a9d6-0fa29bc43769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 20)                5700      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               2100      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 30)                1530      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 14,411\n",
            "Trainable params: 14,411\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDDNtJzgg9p7",
        "colab_type": "code",
        "outputId": "10440031-ffa4-4779-ece0-eeb75a4e224a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#deep_train_predict = model.predict(x_train)\n",
        "deep_test_predict = model.predict(x_test)\n",
        "#print(\"MAPE value of DeepL train:\",mape_find(y_train,deep_train_predict))\n",
        "#print(\"MAPE value of DeepL test:\",mape_find(y_test,deep_test_predict))\n",
        "print('RMSE',np.sqrt(metrics.mean_squared_error(y_test,deep_test_predict)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE 35.72078602039825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}